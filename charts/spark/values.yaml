# Default values for spark.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: "spark"

rbac:
  create: true

image:
  pullPolicy: IfNotPresent
  repository: "harbor-us-west-2.c2fo.com/c2fo/spark-de"
  tag: "3.1.1"
sparkApplication:
  type: Scala
  mode: cluster
  mainApplicationFile: ""
  version: "3.1.1"

  # Log4j is passed here, it's not a completed solution yet. https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/issues/753
  volumes:
    - name: config-vol
      configMap:
        name: spark-log4j-conf-map
  volumeMounts:
    - name: config-vol
      mountPath: /opt/spark/log4j

  # Prometheus metrics that will be pulled by datadog agent
  monitoring:
    exposeDriverMetrics: true
    exposeExecutorMetrics: true
    prometheus:
      jmxExporterJar: ""
      port: 8090
  restartPolicy:
    type: Never
  conf: {}
  # Common environment variables for all the apps
  commonEnvs: {}
  # Common environment variables secret key refs for all the apps
  commonEnvSecretKeyRefs: {}

## The app values ex:
#  app1:
#    fullnameOverride: ""
#    mainClass: ""
#    arguments: []
#    conf: {}
#    driver:
#      javaOptions: ""
#      cores: 1
#      coreLimit: ""
#      memory: "4g"
#      memoryOverhead: "1g"
#    executor:
#      javaOptions: ""
#      cores: 1
#      coreLimit: "1200m"
#      memory: "8g"
#      memoryOverhead: "2g"
#    envs: {}
#    envSecretKeyRefs: {}
#    dynamicAllocation:
#      enabled: true
#      initialExecutors: 1
#      minExecutors: 1
#      maxExecutors: 10
apps: {}

# for testing purpose
service:
  type: ClusterIP
  port: 80